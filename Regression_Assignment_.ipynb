{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Regression Assignment Questions***"
      ],
      "metadata": {
        "id": "4IWVtlHLS5ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Simple Linear Regression?\n",
        "\n",
        "- **Simple Linear Regression** is a statistical method used to model the relationship between two continuous variables:\n",
        "\n",
        "* **Independent variable (X)** ‚Äî also called the *predictor* or *input*.\n",
        "* **Dependent variable (Y)** ‚Äî also called the *response* or *output*.\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "To find the best-fitting **straight line** (also called a regression line) through the data points that predicts Y from X.\n",
        "\n",
        "---\n",
        "\n",
        "### Mathematical Equation:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X + \\varepsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = predicted value\n",
        "* $\\beta_0$ = intercept (value of Y when X = 0)\n",
        "* $\\beta_1$ = slope (change in Y for a one-unit change in X)\n",
        "* $\\varepsilon$ = error term (difference between observed and predicted values)\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "If you're trying to predict a person's weight (Y) based on their height (X), simple linear regression will find a line that best explains how weight changes as height increases.\n",
        "\n",
        "---\n",
        "\n",
        "### Assumptions:\n",
        "\n",
        "1. Linearity ‚Äì the relationship between X and Y is linear.\n",
        "2. Independence ‚Äì observations are independent.\n",
        "3. Homoscedasticity ‚Äì equal variance of errors.\n",
        "4. Normality ‚Äì errors are normally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "### Use Cases:\n",
        "\n",
        "* Predicting sales based on advertising spend\n",
        "* Estimating temperature based on altitude\n",
        "* Forecasting stock prices from historical trends\n",
        "\n"
      ],
      "metadata": {
        "id": "IRJAuMb-TDOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "- Simple Linear Regression relies on several key assumptions to ensure that the model produces valid, reliable, and interpretable results. Here are the **five core assumptions**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Linearity**\n",
        "\n",
        "* **What it means**: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "* **Why it matters**: If the relationship isn't linear, predictions will be systematically off.\n",
        "* **How to check**: Plot a scatterplot of X vs. Y and look for a straight-line pattern.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Independence of Errors**\n",
        "\n",
        "* **What it means**: Observations (and their residuals) are independent of each other.\n",
        "* **Why it matters**: Violations (like autocorrelation in time series) can lead to misleading significance tests.\n",
        "* **How to check**: Use the **Durbin-Watson test** or inspect residual plots (especially over time).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Homoscedasticity (Constant Variance of Errors)**\n",
        "\n",
        "* **What it means**: The variance of residuals (errors) is the same across all levels of X.\n",
        "* **Why it matters**: Unequal variance (heteroscedasticity) can make confidence intervals and significance tests invalid.\n",
        "* **How to check**: Plot residuals vs. predicted values. Look for a consistent spread.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Normality of Errors**\n",
        "\n",
        "* **What it means**: The residuals (errors) are normally distributed.\n",
        "* **Why it matters**: Needed for valid hypothesis testing (e.g., t-tests, confidence intervals).\n",
        "* **How to check**: Use a histogram or Q-Q plot of residuals, or statistical tests like the **Shapiro-Wilk** test.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **No Multicollinearity** (not relevant for **simple** linear regression)\n",
        "\n",
        "* This is important for **multiple** linear regression, not simple regression (since there's only one X variable in simple regression).\n",
        "\n"
      ],
      "metadata": {
        "id": "1E3A5_heTQeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "- In the linear equation:\n",
        "\n",
        "$$\n",
        "Y = mX + c\n",
        "$$\n",
        "\n",
        "the coefficient **$m$** represents the **slope** of the line.\n",
        "\n",
        "---\n",
        "\n",
        "### Meaning of $m$ (Slope):\n",
        "\n",
        "* It indicates the **rate of change** in $Y$ for every one-unit increase in $X$.\n",
        "* In other words:\n",
        "\n",
        "  > **\"How much does $Y$ change when $X$ increases by 1?\"**\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation Examples:\n",
        "\n",
        "1. **If $m = 2$**:\n",
        "\n",
        "   * For every 1 unit increase in $X$, $Y$ increases by 2 units.\n",
        "\n",
        "2. **If $m = -3$**:\n",
        "\n",
        "   * For every 1 unit increase in $X$, $Y$ decreases by 3 units.\n",
        "\n"
      ],
      "metadata": {
        "id": "lsw69OBPTefY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        " - In the linear equation $Y = mX + c$, the term $c$ represents the **intercept**, specifically the value of $Y$ when $X = 0$. This means it is the point where the line crosses the Y-axis. The intercept provides a baseline or starting value for the dependent variable when there is no contribution from the independent variable. For example, in a real-world scenario like predicting monthly electricity costs based on air conditioner usage, the intercept would represent the fixed cost you pay even if the air conditioner isn't used at all. It‚Äôs an important component of the model as it allows the regression line to adjust vertically to best fit the data.\n"
      ],
      "metadata": {
        "id": "qiyVvYdSW7Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "- In **Simple Linear Regression**, the **slope $m$** (often denoted as $\\beta_1$) is calculated using a formula that measures how much the dependent variable $Y$ changes for a one-unit change in the independent variable $X$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Formula to Calculate the Slope $m$:**\n",
        "\n",
        "$$\n",
        "m = \\frac{n \\sum (XY) - \\sum X \\sum Y}{n \\sum X^2 - (\\sum X)^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $n$ = number of data points\n",
        "* $\\sum XY$ = sum of the product of X and Y values\n",
        "* $\\sum X$ = sum of all X values\n",
        "* $\\sum Y$ = sum of all Y values\n",
        "* $\\sum X^2$ = sum of squared X values\n",
        "\n",
        "---\n",
        "\n",
        "### **Alternate (Conceptual) Form:**\n",
        "\n",
        "$$\n",
        "m = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* Cov(X, Y) is the **covariance** between X and Y\n",
        "* Var(X) is the **variance** of X\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps to Calculate $m$:**\n",
        "\n",
        "1. Compute the mean of X and Y: $\\bar{X}$, $\\bar{Y}$\n",
        "2. For each data point, calculate $(X_i - \\bar{X})(Y_i - \\bar{Y})$ and $(X_i - \\bar{X})^2$\n",
        "3. Sum those values and plug into:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Given points (1, 2), (2, 3), (3, 5):\n",
        "\n",
        "* $\\bar{X} = 2$, $\\bar{Y} = 3.33$\n",
        "* Calculate slope using the above steps\n",
        "\n",
        "---\n",
        "\n",
        "The slope tells you how strongly and in what direction X influences Y. Let me know if you want a full numerical example or code to calculate it.\n"
      ],
      "metadata": {
        "id": "gcs8LPXDXOgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "- The **purpose of the least squares method** in Simple Linear Regression is to find the **best-fitting straight line** through a set of data points by **minimizing the sum of the squared differences** between the observed values and the values predicted by the line.\n",
        "\n",
        "In other words, it aims to make the line as close as possible to all the actual data points. These differences are called **residuals** (errors), and by squaring them and summing them up, the method ensures that both positive and negative errors are treated equally, and larger errors are penalized more heavily.\n",
        "\n",
        "The result of the least squares method is the line $Y = mX + c$ where the values of **slope (m)** and **intercept (c)** minimize the total squared error. This makes it the most accurate linear representation of the relationship between the variables under the assumptions of linear regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "2rjTzHINXd0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "- In **Simple Linear Regression**, the **coefficient of determination**, denoted as **$R^2$**, measures how well the regression line explains the variability of the dependent variable $Y$ based on the independent variable $X$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of $R^2$:**\n",
        "\n",
        "* **$R^2$ ranges from 0 to 1**:\n",
        "\n",
        "  * **$R^2 = 1$** ‚Üí Perfect fit: all data points lie exactly on the regression line.\n",
        "  * **$R^2 = 0$** ‚Üí No explanatory power: the line doesn't explain any of the variation in $Y$.\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "  * If $R^2 = 0.85$, it means **85% of the variability** in the dependent variable $Y$ can be explained by the independent variable $X$. The remaining **15%** is due to other factors or random variation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Formula for $R^2$:**\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\text{SS}_{\\text{res}}$ = sum of squares of residuals (errors)\n",
        "* $\\text{SS}_{\\text{tot}}$ = total sum of squares (variation in Y from the mean)\n",
        "\n",
        "---\n",
        "\n",
        "### **Why it Matters:**\n",
        "\n",
        "* $R^2$ helps assess the **goodness of fit** of the model.\n",
        "* Higher $R^2$ generally means better predictive accuracy.\n",
        "* However, a high $R^2$ does **not** guarantee that the model is appropriate or that there's a causal relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "xzr0-0l6Xm10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "- **Multiple Linear Regression** is a statistical technique used to model the relationship between a single dependent variable (**Y**) and **two or more independent variables** (**X‚ÇÅ, X‚ÇÇ, ..., X‚Çô**). It extends simple linear regression, which involves only one independent variable, to handle more complex real-world scenarios where multiple factors influence the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Equation:**\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\varepsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = dependent variable\n",
        "* $X_1, X_2, ..., X_n$ = independent variables (predictors)\n",
        "* $\\beta_0$ = intercept\n",
        "* $\\beta_1, \\beta_2, ..., \\beta_n$ = coefficients showing the effect of each predictor\n",
        "* $\\varepsilon$ = error term (unexplained variation)\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose:**\n",
        "\n",
        "The goal is to estimate how each independent variable affects the dependent variable while **controlling for the influence of the others**. This allows for more accurate predictions and a deeper understanding of the relationships between variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Predicting **house price (Y)** based on:\n",
        "\n",
        "* Size of the house ($X_1$)\n",
        "* Number of bedrooms ($X_2$)\n",
        "* Location score ($X_3$)\n",
        "\n",
        "The model would estimate how much each of these factors contributes to the price.\n",
        "\n",
        "---\n",
        "\n",
        "### **Applications:**\n",
        "\n",
        "* Economics: modeling income based on education, experience, and location\n",
        "* Medicine: predicting disease risk from age, lifestyle, and genetics\n",
        "* Business: forecasting sales from advertising, seasonality, and pricing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eaO_6A6KX-ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "- The **main difference** between **Simple** and **Multiple Linear Regression** lies in the **number of independent variables** used to predict the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Simple Linear Regression:**\n",
        "\n",
        "* **Uses one independent variable** (X) to predict a dependent variable (Y).\n",
        "* **Equation**:\n",
        "\n",
        "  $$\n",
        "  Y = mX + c\n",
        "  $$\n",
        "* **Example**: Predicting a person‚Äôs weight (Y) based on height (X).\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Multiple Linear Regression:**\n",
        "\n",
        "* **Uses two or more independent variables** (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) to predict a dependent variable (Y).\n",
        "* **Equation**:\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n + \\varepsilon\n",
        "  $$\n",
        "* **Example**: Predicting house price (Y) based on size (X‚ÇÅ), number of rooms (X‚ÇÇ), and location (X‚ÇÉ).\n",
        "\n"
      ],
      "metadata": {
        "id": "juSuKpduYSiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "- The key assumptions of **Multiple Linear Regression** ensure that the model provides reliable, unbiased, and interpretable results. These assumptions extend those of simple linear regression to accommodate multiple predictors:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Linearity**\n",
        "\n",
        "* The relationship between each independent variable and the dependent variable is **linear**.\n",
        "* The combined effect of predictors on the outcome is additive and linear.\n",
        "\n",
        "### 2. **Independence of Errors**\n",
        "\n",
        "* The residuals (errors) are independent across observations.\n",
        "* No correlation between the errors of different observations (important especially in time series data).\n",
        "\n",
        "### 3. **Homoscedasticity**\n",
        "\n",
        "* The variance of residuals is constant across all levels of the independent variables.\n",
        "* No patterns in residual spread when plotted against predicted values or predictors.\n",
        "\n",
        "### 4. **Normality of Errors**\n",
        "\n",
        "* The residuals are normally distributed.\n",
        "* Especially important for hypothesis testing and confidence intervals.\n",
        "\n",
        "### 5. **No Multicollinearity**\n",
        "\n",
        "* The independent variables are **not highly correlated** with each other.\n",
        "* High multicollinearity inflates standard errors and makes coefficient estimates unstable and hard to interpret.\n",
        "* Checked using Variance Inflation Factor (VIF) or correlation matrices.\n",
        "\n",
        "### 6. **No Significant Outliers or Influential Points**\n",
        "\n",
        "* Outliers or influential data points can disproportionately affect the regression results.\n",
        "* Should be identified and addressed as necessary.\n",
        "\n"
      ],
      "metadata": {
        "id": "S9VNtix8YgI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "- **Heteroscedasticity** refers to a situation in regression analysis where the **variance of the residuals (errors) is not constant** across all levels of the independent variables. In other words, the spread or \"scatter\" of the errors changes depending on the value of the predictors or the fitted values.\n",
        "\n",
        "---\n",
        "\n",
        "### What does that mean?\n",
        "\n",
        "* Instead of residuals being randomly and evenly spread around zero (which is called **homoscedasticity**), the residuals show a pattern ‚Äî they might fan out, funnel, or cluster more widely at some values of the predictors.\n",
        "* For example, errors might be small for low values of $X$ but much larger for high values of $X$.\n",
        "\n",
        "---\n",
        "\n",
        "### How does heteroscedasticity affect Multiple Linear Regression results?\n",
        "\n",
        "1. **Inefficient estimates**: While the regression coefficients (betas) remain unbiased, the presence of heteroscedasticity causes the standard errors to be incorrect.\n",
        "2. **Invalid statistical inference**: Incorrect standard errors lead to unreliable confidence intervals and hypothesis tests (t-tests, F-tests). This means you might wrongly conclude that a predictor is significant (or not).\n",
        "3. **Poor prediction accuracy**: The model might perform poorly for certain ranges of data where the variance is higher.\n",
        "\n",
        "---\n",
        "\n",
        "### How to detect heteroscedasticity?\n",
        "\n",
        "* Plot residuals versus predicted values or independent variables ‚Äî look for patterns or a \"fanning\" shape.\n",
        "* Statistical tests like **Breusch-Pagan test** or **White test**.\n",
        "\n",
        "---\n",
        "\n",
        "### How to handle heteroscedasticity?\n",
        "\n",
        "* Use **robust standard errors** (also called heteroscedasticity-consistent standard errors).\n",
        "* Transform the dependent variable (e.g., log transformation).\n",
        "* Use weighted least squares regression.\n",
        "* Try different modeling approaches.\n",
        "\n"
      ],
      "metadata": {
        "id": "eHeX4FwiY0rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "- If a Multiple Linear Regression model suffers from **high multicollinearity** (when independent variables are highly correlated), it can make coefficient estimates unstable and hard to interpret. Here are some effective ways to improve the model:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Remove or Combine Correlated Predictors**\n",
        "\n",
        "* Identify highly correlated variables using correlation matrices or Variance Inflation Factor (VIF).\n",
        "* Remove one of the correlated variables if it‚Äôs redundant.\n",
        "* Combine correlated variables into a single predictor using techniques like **Principal Component Analysis (PCA)** or creating an index.\n",
        "\n",
        "### 2. **Regularization Techniques**\n",
        "\n",
        "* Use regression methods that can handle multicollinearity, such as:\n",
        "\n",
        "  * **Ridge Regression** (adds L2 penalty)\n",
        "  * **Lasso Regression** (adds L1 penalty and can perform variable selection)\n",
        "* These methods shrink coefficients and reduce variance caused by multicollinearity.\n",
        "\n",
        "### 3. **Collect More Data**\n",
        "\n",
        "* Increasing sample size can sometimes reduce the impact of multicollinearity.\n",
        "\n",
        "### 4. **Center or Standardize Variables**\n",
        "\n",
        "* Mean-centering (subtracting the mean) or standardizing variables can reduce multicollinearity, especially when interaction terms are involved.\n",
        "\n",
        "### 5. **Check Model Specification**\n",
        "\n",
        "* Re-examine if all variables are necessary or if the model can be simplified.\n",
        "\n"
      ],
      "metadata": {
        "id": "kI5_ZBsVZLXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "- When using categorical variables in regression models, you need to **transform** them into numerical form because regression algorithms require numerical input. Here are some common techniques for doing this:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **One-Hot Encoding (Dummy Variables)**\n",
        "\n",
        "* Converts each category of a categorical variable into a new binary (0/1) variable.\n",
        "* Example: A \"Color\" variable with categories {Red, Blue, Green} becomes three variables: Is\\_Red, Is\\_Blue, Is\\_Green.\n",
        "* Use **k-1 dummy variables** to avoid the \"dummy variable trap\" (multicollinearity).\n",
        "\n",
        "### 2. **Label Encoding**\n",
        "\n",
        "* Assigns an integer value to each category (e.g., Red=1, Blue=2, Green=3).\n",
        "* Simple but can introduce unintended ordinal relationships; better for ordinal categories.\n",
        "\n",
        "### 3. **Ordinal Encoding**\n",
        "\n",
        "* For categorical variables with a natural order (e.g., Low, Medium, High), assign integers that respect the order.\n",
        "* Preserves the rank but assumes equal spacing between categories.\n",
        "\n",
        "### 4. **Binary Encoding**\n",
        "\n",
        "* Converts categories into binary code and splits the digits into separate columns.\n",
        "* Useful for high-cardinality categorical variables (many unique categories) to reduce dimensionality compared to one-hot.\n",
        "\n",
        "### 5. **Frequency Encoding**\n",
        "\n",
        "* Replace categories with their frequency counts or proportions in the dataset.\n",
        "* Captures category prevalence but may lose some interpretability.\n",
        "\n",
        "### 6. **Target Encoding (Mean Encoding)**\n",
        "\n",
        "* Replace categories with the average target value for that category.\n",
        "* Can improve predictive power but risks target leakage and overfitting if not carefully done (usually combined with cross-validation).\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "NuefvoYvaoNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "- In Multiple Linear Regression, **interaction terms** capture the situation where the effect of one independent variable on the dependent variable **depends on the level of another independent variable**. In other words, interaction terms allow the model to account for **combined effects** between variables that aren‚Äôt simply additive.\n",
        "\n",
        "---\n",
        "\n",
        "### What does that mean?\n",
        "\n",
        "* Normally, the model assumes each predictor influences the outcome independently and additively.\n",
        "* But sometimes, the impact of one predictor changes depending on the value of another predictor.\n",
        "* Interaction terms model this by multiplying the two variables together.\n",
        "\n",
        "---\n",
        "\n",
        "### Mathematical form:\n",
        "\n",
        "If you have two variables, $X_1$ and $X_2$, the regression model with an interaction term looks like:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\varepsilon\n",
        "$$\n",
        "\n",
        "Here, $\\beta_3$ measures how the relationship between $X_1$ and $Y$ changes as $X_2$ changes (and vice versa).\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose you‚Äôre modeling plant growth $Y$ based on fertilizer amount $X_1$ and sunlight hours $X_2$.\n",
        "\n",
        "* Without interaction, the model assumes fertilizer and sunlight independently add to growth.\n",
        "* But maybe fertilizer is more effective **only** when sunlight is abundant.\n",
        "* The interaction term $X_1 \\times X_2$ captures this combined effect.\n",
        "\n",
        "---\n",
        "\n",
        "### Why are interaction terms important?\n",
        "\n",
        "* They provide a more flexible, realistic model of relationships.\n",
        "* Help uncover synergies or dependencies between predictors.\n",
        "* Improve model accuracy and interpretation when interactions exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "qQ_bqinnazmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "- Great question! The **interpretation of the intercept** differs subtly between **Simple** and **Multiple Linear Regression** because of the number of predictors involved.\n",
        "\n",
        "---\n",
        "\n",
        "### In **Simple Linear Regression**:\n",
        "\n",
        "* The intercept ($c$ or $\\beta_0$) represents the expected value of the dependent variable $Y$ **when the single independent variable $X$ is zero**.\n",
        "* In other words, it‚Äôs the point where the regression line crosses the Y-axis.\n",
        "* Example: If you‚Äôre predicting weight based on height, the intercept is the predicted weight when height = 0 (which may or may not be meaningful depending on the context).\n",
        "\n",
        "---\n",
        "\n",
        "### In **Multiple Linear Regression**:\n",
        "\n",
        "* The intercept ($\\beta_0$) represents the expected value of $Y$ **when all independent variables $X_1, X_2, ..., X_n$ are zero simultaneously**.\n",
        "* This can be more complex because the combination of all predictors being zero might be unrealistic or outside the data range.\n",
        "* Therefore, the intercept often serves more as a baseline reference rather than a practically meaningful value.\n",
        "* Sometimes, centering the predictors (subtracting their means) can make the intercept more interpretable ‚Äî it then represents the expected $Y$ when all predictors are at their average values.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Caj1LYUa_js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "- The **slope** in regression analysis represents the **rate of change** of the dependent variable $Y$ with respect to a one-unit increase in an independent variable $X$. Essentially, it tells you **how much $Y$ is expected to increase or decrease when $X$ increases by one unit**, assuming all other factors remain constant.\n",
        "\n",
        "---\n",
        "\n",
        "### Significance of the slope:\n",
        "\n",
        "* It quantifies the **strength and direction** of the relationship between $X$ and $Y$.\n",
        "\n",
        "  * A **positive slope** means $Y$ increases as $X$ increases.\n",
        "  * A **negative slope** means $Y$ decreases as $X$ increases.\n",
        "* The magnitude shows how sensitive $Y$ is to changes in $X$.\n",
        "* It helps in understanding and interpreting the influence of predictors on the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "### How the slope affects predictions:\n",
        "\n",
        "* The slope determines the **steepness** of the regression line.\n",
        "* When you plug a new $X$ value into the regression equation, the slope scales this value to estimate $Y$.\n",
        "* Larger absolute values of the slope mean that small changes in $X$ produce larger changes in predicted $Y$.\n",
        "* If the slope is zero, changes in $X$ have no effect on $Y$, meaning no predictive relationship.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "In the equation $Y = 2X + 5$:\n",
        "\n",
        "* The slope is 2, meaning for every 1-unit increase in $X$, $Y$ increases by 2 units.\n",
        "* If $X$ goes from 3 to 4, predicted $Y$ goes from $2*3 + 5 = 11$ to $2*4 + 5 = 13$.\n",
        "\n"
      ],
      "metadata": {
        "id": "q3vGYwrRbH9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "- The **intercept** in a regression model provides a crucial **baseline or reference point** for understanding the relationship between variables. It represents the predicted value of the dependent variable $Y$ **when all independent variables are zero**.\n",
        "\n",
        "---\n",
        "\n",
        "### How the intercept provides context:\n",
        "\n",
        "1. **Baseline value:**\n",
        "   The intercept sets the starting level of $Y$ before considering the influence of any predictors. It tells you where the regression line crosses the Y-axis.\n",
        "\n",
        "2. **Reference for changes:**\n",
        "   By knowing the intercept, you can interpret how changes in the independent variables cause deviations from this baseline.\n",
        "\n",
        "3. **Meaning depends on zero-point:**\n",
        "   The interpretability of the intercept depends on whether having zero values for the predictors makes sense in the context:\n",
        "\n",
        "   * If zero is a meaningful or possible value (e.g., zero hours studied), the intercept is directly interpretable.\n",
        "   * If zero is outside the data range or not meaningful (e.g., zero age), the intercept mainly acts as a mathematical anchor without practical meaning.\n",
        "\n",
        "4. **Helps in centering data:**\n",
        "   Sometimes, predictors are centered (subtracting their means), so the intercept then represents the expected $Y$ at average values of predictors, making interpretation easier.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "If you model a person‚Äôs salary based on years of experience, the intercept is the predicted salary when experience is zero ‚Äî essentially the starting salary. This gives context to how experience affects salary beyond that starting point.\n",
        "\n"
      ],
      "metadata": {
        "id": "inXKcJ-RbVxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "- Great question! While **$R^2$** (coefficient of determination) is a popular measure of how well a regression model explains the variability of the dependent variable, it has several important **limitations** if used alone to judge model performance:\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations of using $R^2$ as the sole measure:\n",
        "\n",
        "1. **Doesn‚Äôt indicate causation or model correctness:**\n",
        "   A high $R^2$ just means the model fits the data well, but it doesn‚Äôt prove that predictors cause changes in the outcome or that the model assumptions are valid.\n",
        "\n",
        "2. **Insensitive to overfitting:**\n",
        "   $R^2$ always increases (or at least doesn‚Äôt decrease) when you add more variables‚Äîeven if those variables are irrelevant. This can encourage overly complex models.\n",
        "\n",
        "3. **No penalty for complexity:**\n",
        "   Unlike adjusted $R^2$ or other metrics, plain $R^2$ doesn‚Äôt penalize for having many predictors, so it can be misleading when comparing models with different numbers of variables.\n",
        "\n",
        "4. **Doesn‚Äôt reflect predictive accuracy on new data:**\n",
        "   A model with a high $R^2$ on training data may perform poorly on unseen data. $R^2$ does not measure out-of-sample prediction quality.\n",
        "\n",
        "5. **Not meaningful for non-linear or non-constant variance models:**\n",
        "   $R^2$ assumes linear relationships and constant variance; if these assumptions fail, $R^2$ can be misleading.\n",
        "\n",
        "6. **Can be low even if model is useful:**\n",
        "   In some cases, especially with noisy data, $R^2$ can be low even though the model provides valuable insights or predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### What to use along with $R^2$:\n",
        "\n",
        "* **Adjusted $R^2$:** Penalizes for extra predictors, better for model comparison.\n",
        "* **Residual analysis:** Check assumptions and fit quality.\n",
        "* **Cross-validation metrics:** Such as RMSE, MAE on test data.\n",
        "* **Statistical tests and domain knowledge:** To verify model validity.\n",
        "\n"
      ],
      "metadata": {
        "id": "_sufLM5hbfqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "- A **large standard error** for a regression coefficient indicates **greater uncertainty** or variability in the estimate of that coefficient. Here's what that means in practice:\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation of a large standard error for a coefficient:\n",
        "\n",
        "1. **Less precise estimate:**\n",
        "   The coefficient‚Äôs value is estimated with low precision, meaning the true effect size could vary widely around the estimated value.\n",
        "\n",
        "2. **Wide confidence intervals:**\n",
        "   Large standard errors lead to wider confidence intervals for the coefficient, making it harder to be confident about the exact effect size.\n",
        "\n",
        "3. **Lower statistical significance:**\n",
        "   Because the coefficient estimate is uncertain, the corresponding t-statistic (coefficient divided by its standard error) may be small, often resulting in a **high p-value**. This suggests the predictor may **not be statistically significant** in explaining the dependent variable.\n",
        "\n",
        "4. **Potential causes:**\n",
        "\n",
        "   * **Multicollinearity:** Highly correlated predictors inflate standard errors.\n",
        "   * **Small sample size:** Limited data reduces estimate reliability.\n",
        "   * **High variability in data:** Noisy or inconsistent data increases uncertainty.\n",
        "   * **Model misspecification:** Important variables missing or wrong functional form.\n",
        "\n"
      ],
      "metadata": {
        "id": "PDeFgZRwbsSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "- Great question! Here's how you can **identify heteroscedasticity** in residual plots and why addressing it matters:\n",
        "\n",
        "---\n",
        "\n",
        "### How to identify heteroscedasticity in residual plots:\n",
        "\n",
        "* **Plot residuals vs. predicted values (fitted values) or vs. an independent variable.**\n",
        "* If the **spread (variance) of residuals remains roughly constant** across all levels of predicted values, that's **homoscedasticity** (good).\n",
        "* If the residuals show a **pattern where the spread increases or decreases**‚Äîlike a funnel, cone, or fan shape‚Äîthis indicates **heteroscedasticity**.\n",
        "* You might also see clusters or systematic changes in variance, rather than random scatter.\n",
        "\n",
        "---\n",
        "\n",
        "### Why is it important to address heteroscedasticity?\n",
        "\n",
        "1. **Invalid standard errors:**\n",
        "   Heteroscedasticity causes the usual regression assumptions to break down, making standard errors of coefficients unreliable.\n",
        "\n",
        "2. **Faulty inference:**\n",
        "   Because standard errors are biased, confidence intervals and hypothesis tests (like t-tests for coefficients) may be misleading, leading to incorrect conclusions.\n",
        "\n",
        "3. **Inefficient estimates:**\n",
        "   Although coefficients remain unbiased, the model is no longer the **best linear unbiased estimator (BLUE)**, meaning estimates may be less precise.\n",
        "\n",
        "4. **Poor prediction:**\n",
        "   The model might under- or overestimate uncertainty in certain regions, reducing prediction quality.\n",
        "\n",
        "\n",
        "### What to do about it?\n",
        "\n",
        "* Use **robust (heteroscedasticity-consistent) standard errors**.\n",
        "* Transform the dependent variable (e.g., log transform).\n",
        "* Use weighted least squares.\n",
        "* Explore alternative models.\n",
        "\n"
      ],
      "metadata": {
        "id": "nRDezKKPb5L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "- If a **Multiple Linear Regression** model has a **high $R^2$** but a **low adjusted $R^2$**, it usually means that:\n",
        "\n",
        "---\n",
        "\n",
        "### What it means:\n",
        "\n",
        "* The model explains a large proportion of the variance in the dependent variable when considering **all predictors** (high $R^2$).\n",
        "* However, **after adjusting for the number of predictors and sample size**, the model‚Äôs explanatory power doesn‚Äôt improve much‚Äîor may even decrease (low adjusted $R^2$).\n",
        "* This suggests the model may be **overfitting** by including too many predictors that don‚Äôt actually contribute meaningful information.\n",
        "\n",
        "---\n",
        "\n",
        "### Why does this happen?\n",
        "\n",
        "* $R^2$ **always increases or stays the same** when you add more variables, even if those variables are irrelevant.\n",
        "* Adjusted $R^2$ **penalizes adding unnecessary predictors** by adjusting for degrees of freedom.\n",
        "* A big gap (high $R^2$, low adjusted $R^2$) signals that some predictors may be noise or redundant.\n",
        "\n",
        "---\n",
        "\n",
        "### What should you do?\n",
        "\n",
        "* Consider **removing non-significant or irrelevant variables**.\n",
        "* Use **feature selection techniques** (e.g., stepwise regression, Lasso).\n",
        "* Evaluate models using cross-validation or other performance metrics.\n",
        "* Focus on simpler, more interpretable models that generalize better.\n",
        "\n"
      ],
      "metadata": {
        "id": "hoUW9FjncKcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "- Scaling variables in Multiple Linear Regression is important for several reasons, especially when your predictors have very different units or ranges:\n",
        "\n",
        "---\n",
        "\n",
        "### Why scaling matters:\n",
        "\n",
        "1. **Improves numerical stability:**\n",
        "   Variables with vastly different scales (e.g., age in years vs. income in thousands) can cause computational issues or reduce the precision of the model fitting process.\n",
        "\n",
        "2. **Makes coefficients comparable:**\n",
        "   Scaling puts all predictors on a similar scale, so the magnitude of coefficients reflects the relative importance of variables more meaningfully.\n",
        "\n",
        "3. **Helps optimization algorithms converge faster:**\n",
        "   Many regression algorithms (especially those using gradient descent or regularization like Ridge/Lasso) perform better and converge quicker when variables are scaled.\n",
        "\n",
        "4. **Essential for regularization:**\n",
        "   Regularization methods (Lasso, Ridge) penalize coefficients; if predictors aren‚Äôt scaled, variables with larger ranges can dominate the penalty unfairly.\n",
        "\n",
        "5. **Easier interpretation of interaction terms and polynomial features:**\n",
        "   When creating interaction or polynomial terms, scaling helps avoid extremely large values that can skew the model.\n",
        "\n",
        "---\n",
        "\n",
        "### Common scaling methods:\n",
        "\n",
        "* **Standardization (Z-score):** Subtract mean and divide by standard deviation; results in variables with mean 0 and variance 1.\n",
        "* **Min-max scaling:** Rescales variables to a fixed range, usually \\[0,1].\n",
        "\n"
      ],
      "metadata": {
        "id": "Szw8DmTecW_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "- **Polynomial regression** is an extension of linear regression where the relationship between the independent variable $X$ and the dependent variable $Y$ is modeled as an **nth-degree polynomial** rather than a straight line.\n",
        "\n",
        "---\n",
        "\n",
        "### What does that mean?\n",
        "\n",
        "* Instead of fitting a line $Y = \\beta_0 + \\beta_1 X$, polynomial regression fits a curve:\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\varepsilon\n",
        "  $$\n",
        "\n",
        "* This allows the model to capture **non-linear relationships** between $X$ and $Y$.\n",
        "\n",
        "---\n",
        "\n",
        "### Why use polynomial regression?\n",
        "\n",
        "* When the relationship between variables is not linear but still smooth and continuous.\n",
        "* To improve fit when simple linear regression underfits the data.\n",
        "* To model curves, bends, or inflection points in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### Key points:\n",
        "\n",
        "* Polynomial regression is still a type of linear regression because it‚Äôs linear in the parameters $\\beta_i$, even though the predictors are powers of $X$.\n",
        "* The degree $n$ controls the flexibility of the model: higher degrees can fit more complex curves but risk overfitting.\n",
        "* It‚Äôs often helpful to visualize data first to decide if polynomial regression makes sense.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "If you suspect a quadratic relationship between study hours $X$ and exam score $Y$, you might fit:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\varepsilon\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "_GPgrNj1cfv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "- Great question! Here‚Äôs how **polynomial regression** differs from **linear regression**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Form of the relationship modeled:**\n",
        "\n",
        "* **Linear Regression:**\n",
        "  Models a **linear relationship** between the dependent variable $Y$ and the independent variable(s) $X$. The equation is:\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "  $$\n",
        "\n",
        "  This means $Y$ changes at a constant rate with $X$.\n",
        "\n",
        "* **Polynomial Regression:**\n",
        "  Models a **non-linear relationship** by including powers of $X$ (like $X^2, X^3,$ etc.):\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\varepsilon\n",
        "  $$\n",
        "\n",
        "  This allows the relationship to be curved or more complex.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Flexibility:**\n",
        "\n",
        "* Linear regression fits a straight line.\n",
        "* Polynomial regression fits a curve, which can capture bends and more complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Interpretation:**\n",
        "\n",
        "* In linear regression, the slope $\\beta_1$ represents the constant change in $Y$ for a one-unit increase in $X$.\n",
        "* In polynomial regression, the effect of $X$ on $Y$ **changes depending on the value of $X$** because of the higher-degree terms.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Model complexity and risk:**\n",
        "\n",
        "* Polynomial regression can fit the training data better by increasing the degree, but risks **overfitting** if the degree is too high.\n",
        "* Linear regression is simpler and less prone to overfitting but may underfit if the true relationship is non-linear.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-kusu88cpNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "- Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is **non-linear** but can be modeled as a smooth curve. Here are common situations where polynomial regression is appropriate:\n",
        "\n",
        "---\n",
        "\n",
        "### When to use polynomial regression:\n",
        "\n",
        "1. **Non-linear trends in data:**\n",
        "   When scatterplots or exploratory analysis show that the data points follow a curved pattern rather than a straight line.\n",
        "\n",
        "2. **Capturing curvature:**\n",
        "   To model phenomena where effects increase or decrease at changing rates ‚Äî for example, acceleration, growth that slows down or speeds up, or cyclical trends.\n",
        "\n",
        "3. **Improving model fit:**\n",
        "   When simple linear regression underfits the data (poorly represents the trend), adding polynomial terms can provide a better fit.\n",
        "\n",
        "4. **Single predictor relationships:**\n",
        "   Often used with one independent variable to capture complex patterns, though it can be extended to multiple predictors by including polynomial terms of each.\n",
        "\n",
        "5. **Engineering, physics, economics, biology:**\n",
        "   Many natural and social processes show non-linear relationships (e.g., projectile motion, dose-response curves, economic growth), where polynomial regression helps model those effects.\n",
        "\n",
        "---\n",
        "\n",
        "### When **not** to use polynomial regression:\n",
        "\n",
        "* If the data relationship is not smooth or has abrupt changes.\n",
        "* If the degree needed is very high (risk of overfitting).\n",
        "* When there are better suited non-linear models or machine learning methods.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "Modeling the growth of plants where initial growth is slow, then speeds up, then slows again can often be captured well by a quadratic or cubic polynomial regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "usKSKEH7czh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "- The general equation for **polynomial regression** of degree $n$ with one independent variable $X$ is:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\varepsilon\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* $Y$ is the dependent variable (what you‚Äôre trying to predict).\n",
        "* $\\beta_0$ is the intercept (the value of $Y$ when $X = 0$).\n",
        "* $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients for each power of $X$.\n",
        "* $X, X^2, X^3, ..., X^n$ are the independent variable raised to powers from 1 up to $n$.\n",
        "* $\\varepsilon$ is the error term (the difference between observed and predicted values).\n",
        "\n"
      ],
      "metadata": {
        "id": "IdNwcOYjc7P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "- Yes, **polynomial regression can be applied to multiple variables**, and this is often called **multivariate polynomial regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### How it works with multiple variables:\n",
        "\n",
        "* You include **polynomial terms** (squares, cubes, etc.) of each independent variable.\n",
        "* You can also include **interaction terms** between variables (e.g., $X_1 \\times X_2$, $X_1^2 \\times X_2$, etc.) to capture more complex relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### General form for two variables $X_1$ and $X_2$ with degree 2 (quadratic):\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\varepsilon\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dksl66ytdF0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "- Polynomial regression is powerful for modeling nonlinear relationships, but it has several important limitations to keep in mind:\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations of Polynomial Regression:\n",
        "\n",
        "1. **Risk of Overfitting:**\n",
        "   Higher-degree polynomials can fit the training data very closely, including noise, which leads to poor generalization on new data.\n",
        "\n",
        "2. **Extrapolation Issues:**\n",
        "   Polynomial models can behave erratically outside the range of the training data, producing unrealistic predictions.\n",
        "\n",
        "3. **Interpretability:**\n",
        "   As the degree increases, the model becomes more complex and harder to interpret meaningfully.\n",
        "\n",
        "4. **Multicollinearity:**\n",
        "   Polynomial terms (e.g., $X$, $X^2$, $X^3$) are often highly correlated, which can inflate standard errors and make coefficient estimates unstable.\n",
        "\n",
        "5. **Model Complexity Grows Quickly:**\n",
        "   With multiple variables and higher degrees, the number of polynomial and interaction terms grows rapidly, increasing computational cost and risk of overfitting.\n",
        "\n",
        "6. **Sensitive to Outliers:**\n",
        "   Polynomials can be disproportionately influenced by outliers, distorting the fit.\n",
        "\n",
        "7. **Not Suitable for All Nonlinear Relationships:**\n",
        "   Some nonlinear patterns may be better modeled by other methods (e.g., splines, generalized additive models, tree-based models) rather than global polynomials.\n",
        "\n"
      ],
      "metadata": {
        "id": "R0VcZ6sidTYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "- When selecting the degree of a polynomial in polynomial regression, it‚Äôs important to evaluate how well the model fits the data without overfitting. Here are some common methods used to evaluate model fit and choose the best polynomial degree:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Cross-Validation (CV)**\n",
        "\n",
        "* **K-fold CV** or **Leave-One-Out CV** helps estimate how well the model generalizes to unseen data.\n",
        "* You compare the average prediction error (e.g., mean squared error) across folds for different polynomial degrees.\n",
        "* Choose the degree with the lowest cross-validation error to balance fit and generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Adjusted $R^2$**\n",
        "\n",
        "* Unlike plain $R^2$, adjusted $R^2$ penalizes the addition of unnecessary polynomial terms.\n",
        "* Increasing the polynomial degree should only improve adjusted $R^2$ if the added complexity genuinely improves the model.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Information Criteria (AIC, BIC)**\n",
        "\n",
        "* Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) balance model fit and complexity.\n",
        "* Lower AIC or BIC indicates a better model. These criteria penalize complex models to avoid overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Residual Analysis**\n",
        "\n",
        "* Plot residuals for different polynomial degrees to check for patterns or heteroscedasticity.\n",
        "* A good fit will show randomly scattered residuals without obvious trends.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Visual Inspection**\n",
        "\n",
        "* Plot the fitted polynomial curve against data points.\n",
        "* Visually assess whether increasing polynomial degree meaningfully improves the fit or just adds unnecessary wiggles.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Test Set Evaluation**\n",
        "\n",
        "* If you have a separate test dataset, evaluate model performance (e.g., RMSE, MAE) on it for different degrees.\n",
        "* The best degree balances good test performance without overfitting the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "jHBoa8X2dc7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "- Visualization is especially important in polynomial regression for several key reasons:\n",
        "\n",
        "---\n",
        "\n",
        "### Why visualization matters in polynomial regression:\n",
        "\n",
        "1. **Understand the relationship shape:**\n",
        "   Polynomial regression models nonlinear relationships, so plotting the fitted curve alongside the data helps you see if the model captures the true pattern or if it‚Äôs overfitting/underfitting.\n",
        "\n",
        "2. **Detect overfitting:**\n",
        "   High-degree polynomials can create wildly oscillating curves that fit training data perfectly but don‚Äôt generalize well. Visualization reveals these unnecessary ‚Äúwiggles‚Äù that numerical metrics alone might not clearly show.\n",
        "\n",
        "3. **Assess model appropriateness:**\n",
        "   Visual plots help verify whether a polynomial is the right choice, or if the data pattern suggests a different model type.\n",
        "\n",
        "4. **Identify outliers and influential points:**\n",
        "   Visualizing residuals and fitted values can highlight data points that disproportionately affect the model, which may require further investigation or treatment.\n",
        "\n",
        "5. **Communicate results:**\n",
        "   Plots provide an intuitive way to explain the model fit and relationship to others, including non-technical stakeholders.\n",
        "\n",
        "---\n",
        "\n",
        "### Common visualizations used:\n",
        "\n",
        "* **Scatter plot with polynomial fit curve** ‚Äî shows how well the polynomial fits the data points.\n",
        "* **Residual plots** ‚Äî reveal patterns in errors indicating model problems.\n",
        "* **Comparison of fits with different polynomial degrees** ‚Äî helps decide the best complexity level visually.\n",
        "\n"
      ],
      "metadata": {
        "id": "LRjtjMXJdoTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.  How is polynomial regression implemented in Python?\n",
        "\n",
        "- Polynomial regression in Python is typically implemented using libraries like scikit-learn. Here‚Äôs a simple step-by-step guide on how to do it:\n",
        "\n",
        "Step 1: Import necessary libraries"
      ],
      "metadata": {
        "id": "kUZI9PKbd27j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n"
      ],
      "metadata": {
        "id": "mPBvlY8DeCBc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Prepare your data"
      ],
      "metadata": {
        "id": "eKpvyNk5eJYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)  # Feature matrix (must be 2D)\n",
        "y = np.array([3, 6, 7, 8, 11, 13, 14, 15, 17])           # Target variable\n"
      ],
      "metadata": {
        "id": "sxCL2-_UeML9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a polynomial regression model"
      ],
      "metadata": {
        "id": "knowHppUeOcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree = 3  # Degree of the polynomial\n",
        "\n",
        "# Create a pipeline that first transforms input to polynomial features, then fits linear regression\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n"
      ],
      "metadata": {
        "id": "I1CpgWk4eQat"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Fit the model"
      ],
      "metadata": {
        "id": "peBCSGgMeUl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "heL3THwdeVQt",
        "outputId": "41bd9881-98a0-4448-a62e-1b7989dc6e3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n",
              "                ('linearregression', LinearRegression())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;polynomialfeatures&#x27;, PolynomialFeatures(degree=3)),\n",
              "                (&#x27;linearregression&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;polynomialfeatures&#x27;, PolynomialFeatures(degree=3)),\n",
              "                (&#x27;linearregression&#x27;, LinearRegression())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>PolynomialFeatures</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">?<span>Documentation for PolynomialFeatures</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>PolynomialFeatures(degree=3)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Make predictions and visualize"
      ],
      "metadata": {
        "id": "ih4YDWoieYEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a range of values for plotting a smooth curve\n",
        "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "y_plot = model.predict(X_plot)\n",
        "\n",
        "# Plot original data and the polynomial regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X_plot, y_plot, color='red', label=f'Polynomial degree {degree}')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "G5HAZxEheafF",
        "outputId": "aa33239b-a061-4a03-861f-fde445e9fa76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}